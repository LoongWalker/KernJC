configurations:
- nodes:
  - cpeMatch:
    - criteria: cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*
      matchCriteriaId: 5C41F9D9-FD77-4351-9E80-31CDB3FB794F
      versionEndExcluding: 6.1.103
      vulnerable: true
    - criteria: cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*
      matchCriteriaId: CC912330-6B41-4C6B-99AF-F3857FBACB6A
      versionEndExcluding: 6.6.44
      versionStartIncluding: '6.2'
      vulnerable: true
    - criteria: cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*
      matchCriteriaId: 92D388F2-1EAF-4CFA-AC06-5B26D762EA7D
      versionEndExcluding: 6.10.3
      versionStartIncluding: '6.7'
      vulnerable: true
    negate: false
    operator: OR
cveTags: []
descriptions:
- lang: en
  value: "In the Linux kernel, the following vulnerability has been resolved:\n\n\
    md: fix deadlock between mddev_suspend and flush bio\n\nDeadlock occurs when mddev\
    \ is being suspended while some flush bio is in\nprogress. It is a complex issue.\n\
    \nT1. the first flush is at the ending stage, it clears 'mddev->flush_bio'\n \
    \   and tries to submit data, but is blocked because mddev is suspended\n    by\
    \ T4.\nT2. the second flush sets 'mddev->flush_bio', and attempts to queue\n \
    \   md_submit_flush_data(), which is already running (T1) and won't\n    execute\
    \ again if on the same CPU as T1.\nT3. the third flush inc active_io and tries\
    \ to flush, but is blocked because\n    'mddev->flush_bio' is not NULL (set by\
    \ T2).\nT4. mddev_suspend() is called and waits for active_io dec to 0 which is\
    \ inc\n    by T3.\n\n  T1\t\tT2\t\tT3\t\tT4\n  (flush 1)\t(flush 2)\t(third 3)\t\
    (suspend)\n  md_submit_flush_data\n   mddev->flush_bio = NULL;\n   .\n   .\t \t\
    md_flush_request\n   .\t  \t mddev->flush_bio = bio\n   .\t  \t queue submit_flushes\n\
    \   .\t\t .\n   .\t\t .\t\tmd_handle_request\n   .\t\t .\t\t active_io + 1\n \
    \  .\t\t .\t\t md_flush_request\n   .\t\t .\t\t  wait !mddev->flush_bio\n   .\t\
    \t .\n   .\t\t .\t\t\t\tmddev_suspend\n   .\t\t .\t\t\t\t wait !active_io\n  \
    \ .\t\t .\n   .\t\t submit_flushes\n   .\t\t queue_work md_submit_flush_data\n\
    \   .\t\t //md_submit_flush_data is already running (T1)\n   .\n   md_handle_request\n\
    \    wait resume\n\nThe root issue is non-atomic inc/dec of active_io during flush\
    \ process.\nactive_io is dec before md_submit_flush_data is queued, and inc soon\n\
    after md_submit_flush_data() run.\n  md_flush_request\n    active_io + 1\n   \
    \ submit_flushes\n      active_io - 1\n      md_submit_flush_data\n        md_handle_request\n\
    \        active_io + 1\n          make_request\n        active_io - 1\n\nIf active_io\
    \ is dec after md_handle_request() instead of within\nsubmit_flushes(), make_request()\
    \ can be called directly intead of\nmd_handle_request() in md_submit_flush_data(),\
    \ and active_io will\nonly inc and dec once in the whole flush process. Deadlock\
    \ will be\nfixed.\n\nAdditionally, the only difference between fixing the issue\
    \ and before is\nthat there is no return error handling of make_request(). But\
    \ after\nprevious patch cleaned md_write_start(), make_requst() only return error\n\
    in raid5_make_request() by dm-raid, see commit 41425f96d7aa (\"dm-raid456,\nmd/raid456:\
    \ fix a deadlock for dm-raid456 while io concurrent with\nreshape)\". Since dm\
    \ always splits data and flush operation into two\nseparate io, io size of flush\
    \ submitted by dm always is 0, make_request()\nwill not be called in md_submit_flush_data().\
    \ To prevent future\nmodifications from introducing issues, add WARN_ON to ensure\n\
    make_request() no error is returned in this context."
- lang: es
  value: "En el kernel de Linux, se resolvi\xF3 la siguiente vulnerabilidad: md: corrige\
    \ el punto muerto entre mddev_suspend y purgar bio. El punto muerto ocurre cuando\
    \ mddev se suspende mientras se realiza alg\xFAn purga de biograf\xEDa. Es una\
    \ cuesti\xF3n compleja. T1. la primera descarga est\xE1 en la etapa final, borra\
    \ 'mddev-&gt;flush_bio' e intenta enviar datos, pero se bloquea porque T4 suspende\
    \ mddev. T2. la segunda descarga establece 'mddev-&gt;flush_bio' e intenta poner\
    \ en cola md_submit_flush_data(), que ya se est\xE1 ejecutando (T1) y no se ejecutar\xE1\
    \ nuevamente si est\xE1 en la misma CPU que T1. T3. el tercer enjuague incluye\
    \ active_io e intenta descargar, pero se bloquea porque 'mddev-&gt;flush_bio'\
    \ no es NULL (establecido por T2). T4. Se llama a mddev_suspend() y espera que\
    \ active_io dec a 0, que es incrementado por T3. T1 T2 T3 T4 (flush 1) (flush\
    \ 2) (tercero 3) (suspender) md_submit_flush_data mddev-&gt;flush_bio = NULL;\
    \ . . md_flush_request. mddev-&gt;flush_bio = biograf\xEDa. cola submit_flushes\
    \ . . . . md_handle_request. . activo_io + 1. . md_flush_request. . \xA1espera!\
    \ mddev-&gt;flush_bio. . . . mddev_suspend. . \xA1espera! active_io. . . enviar_flushes.\
    \ queue_work md_submit_flush_data. //md_submit_flush_data ya se est\xE1 ejecutando\
    \ (T1). md_handle_request espera reanudar la ra\xEDz del problema es el aumento/disminuci\xF3\
    n no at\xF3mico de active_io durante el proceso de descarga. active_io disminuye\
    \ antes de que md_submit_flush_data se ponga en cola y se inc poco despu\xE9s\
    \ de ejecutar md_submit_flush_data(). md_flush_request active_io + 1 submit_flushes\
    \ active_io - 1 md_submit_flush_data md_handle_request active_io + 1 make_request\
    \ active_io - 1 Si active_io se dec despu\xE9s de md_handle_request() en lugar\
    \ de dentro de submit_flushes(), se puede llamar a make_request() directamente\
    \ en lugar de md_handle_request() en md_submit_flush_data(), y active_io solo\
    \ aumentar\xE1 y disminuir\xE1 una vez durante todo el proceso de descarga. Se\
    \ solucionar\xE1 el punto muerto. Adem\xE1s, la \xFAnica diferencia entre solucionar\
    \ el problema y antes es que no hay manejo de errores de devoluci\xF3n de make_request().\
    \ Pero despu\xE9s de que el parche anterior limpi\xF3 md_write_start(), make_requst()\
    \ solo devuelve un error en raid5_make_request() por dm-raid, consulte el commit\
    \ 41425f96d7aa (\"dm-raid456, md/raid456: solucione un punto muerto para dm-raid456\
    \ mientras io concurre con reshape) \". Dado que dm siempre divide los datos y\
    \ la operaci\xF3n de descarga en dos io separados, el tama\xF1o de io de descarga\
    \ enviado por dm siempre es 0, no se llamar\xE1 a make_request() en md_submit_flush_data().\
    \ Para evitar que modificaciones futuras introduzcan problemas, agregue WARN_ON\
    \ para garantizar que make_request() no se devuelva ning\xFAn error en este contexto."
id: CVE-2024-43855
lastModified: '2024-08-22T17:48:09.100'
metrics:
  cvssMetricV31:
  - cvssData:
      attackComplexity: LOW
      attackVector: LOCAL
      availabilityImpact: HIGH
      baseScore: 5.5
      baseSeverity: MEDIUM
      confidentialityImpact: NONE
      integrityImpact: NONE
      privilegesRequired: LOW
      scope: UNCHANGED
      userInteraction: NONE
      vectorString: CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
      version: '3.1'
    exploitabilityScore: 1.8
    impactScore: 3.6
    source: nvd@nist.gov
    type: Primary
published: '2024-08-17T10:15:10.527'
references:
- source: 416baaa9-dc9f-4396-8d5f-8c081fb06d67
  tags:
  - Patch
  url: https://git.kernel.org/stable/c/2d0738a8322bf4e5bfe693d16b3111928a9ccfbf
- source: 416baaa9-dc9f-4396-8d5f-8c081fb06d67
  tags:
  - Patch
  url: https://git.kernel.org/stable/c/32226070813140234b6c507084738e8e8385c5c6
- source: 416baaa9-dc9f-4396-8d5f-8c081fb06d67
  tags:
  - Patch
  url: https://git.kernel.org/stable/c/611d5cbc0b35a752e657a83eebadf40d814d006b
- source: 416baaa9-dc9f-4396-8d5f-8c081fb06d67
  tags:
  - Patch
  url: https://git.kernel.org/stable/c/ca963eefbc3331222b6121baa696d49ba2008811
sourceIdentifier: 416baaa9-dc9f-4396-8d5f-8c081fb06d67
vulnStatus: Analyzed
weaknesses:
- description:
  - lang: en
    value: CWE-476
  source: nvd@nist.gov
  type: Primary
